The data is provided in the form of pointclouds from the semantikitty dataset. We used the provided
function in the colab notebook on PointNet for reading and sampling the pointcloud. It is to note that This function
does not read the full pointcloud but returns a set of points (with fixed size).\par
In order to transform the data we used the open3D api for such task. We decided to use voxelgrids of size $32^3$,
 to have simple calculations (as it is a power of $2$) and a manageable number of voxel to process.
The voxels are encoded into a $32\times32\times32\times1$, where the element $(X,Y,Z,0)$ is set to $1$ iff there exists a voxel with coordinates $(X,Y,Z)$.\par
For the label encoding we decided to encode them as a $32\times32\times32\times\#classes$ tensor as
follows:
for each label $l$ (one-hot encoded) corresponding to a point contained in the voxel with coordinates
$(X,Y,Z)$, the value of the tensor is updated as $labels(X,Y,Z,:)+=l$. After iterating over all the labels the labels tensor is normalized so that the features correspond to a probability distribution. For empty voxels we decided to encode them as a fictitious class. This is done only in order for
the crossEntropy loss to work correcly (as it expects a distribution). This encoding only happens 
at runtime for prediction and loss, as we the algorithm already knows what voxels are empty and
which ones are not.\par
This process takes a lot of time, as for our tests it takes about $1s$ in order to convert
using the open3D function, and 2 seconds at most to convert the labels. This amount of time
is not ideal to have at training time, as it takes most of a training round. 
What we resorted to is to compute and save the voxels beforehand, so at training time 
they would only be loaded, allowing also to have a stable dataset (as the samples from the pointcloud
are not deterministic). In real application this process would need to be made at a lower level, 
as the transformation time would be far too large to have a fast response.
