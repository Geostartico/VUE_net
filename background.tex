The problem considered is the segmentation of urban environments from pointclouds,
In particular we used the Semantikitty dataset with a 1000 samples. 
We started by considering the problem from the pointcloud as the encoding of the 
environment. As suggested we first looked at the architecture of the Pointnet\cite{Pointnet},
in particular this model shows the importance of the data being orientation independent, this
is done through a learnt affine transformation learnt from the distribution of the data and 
applied to all the points uniformly. In particular the affine transformation is learnt to be
almost invertible by making the transformation matrix symmetrical. This is only one of the issues
of working with pointclouds, not to consider how heavy the model is, making it completely unusable for real time applications.\par
The approach we chose is to use voxels to segment the urban environment.
A voxel is a volumetric unit of space, so it can be considered as a pixel in 3D space. For pointclouds
the approach is to create a three dimensional grid, which with each value having a number of
features representing the content of the voxel. What we used is a binary occupancy grid, 
thus for each voxel its value is one iff there exists a point inside of it bounding box.
This was done with the use of the open3d library, which allows for such transformation.\par
Because of the similarity with image encoding we decided to take inspiration for our architecture
from segmentation networks from used for images. One example in particular we took is \cite{Unet}.
What this network does is exploiting a autoencorder-like architecture to obtain the segmented image.
In particular it uses convolution to extract features and maxpooling to downsample the image.
Another element we took inspiration from is the use of concatenation in order to provide
residual connections between the "down" convolution of the image and the up-convolution, it is
to note that the paper uses cropping in order to do these operations as the dimensions do not exactly match, as we will see later our architecture doesn't have this issue as we decided to simplify
this process by having hidden states of equal size.
The last inspiration we took for architecture is \cite{VoxSegNet}. This architecture uses the 
sequential extraction of features to obtain prediction from the use of features from different
stages. This is analogous to the residual, however with some more complexness added that we won't
discuss here. What we took in particular is the use of convolutional layers with different dilation rates in order to get features from different atrous convolutions.
